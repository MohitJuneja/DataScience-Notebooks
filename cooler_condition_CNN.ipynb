{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "import os\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle(\"X.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              PS1         PS2       PS3           PS4       PS5       PS6  \\\n",
       "0     160.673492  109.466914  1.991475  0.000000e+00  9.842170  9.728097   \n",
       "1     160.603320  109.354890  1.976234  0.000000e+00  9.635142  9.529488   \n",
       "2     160.347720  109.158845  1.972224  0.000000e+00  9.530548  9.427949   \n",
       "3     160.188088  109.064807  1.946575  0.000000e+00  9.438827  9.337430   \n",
       "4     160.000472  108.931434  1.922707  0.000000e+00  9.358762  9.260636   \n",
       "5     159.920210  108.887682  1.913284  0.000000e+00  9.301160  9.206877   \n",
       "6     159.672675  108.676466  1.888100  0.000000e+00  9.233942  9.143320   \n",
       "7     159.614452  108.651745  1.874894  0.000000e+00  9.194159  9.105058   \n",
       "8     159.475745  108.529738  1.858120  0.000000e+00  9.144616  9.057067   \n",
       "9     159.437997  108.510885  1.841063  0.000000e+00  9.104831  9.019265   \n",
       "10    159.321757  108.437852  1.832484  0.000000e+00  9.068217  8.983758   \n",
       "11    159.235113  108.316782  1.825662  0.000000e+00  9.023034  8.940413   \n",
       "12    159.135148  108.241080  1.815895  0.000000e+00  8.981117  8.901621   \n",
       "13    159.087352  108.214037  1.807515  0.000000e+00  8.948214  8.870687   \n",
       "14    158.985823  108.127403  1.803809  0.000000e+00  8.915401  8.840756   \n",
       "15    158.864240  108.038512  1.802399  0.000000e+00  8.890655  8.816202   \n",
       "16    158.819642  107.981409  1.793656  5.000000e-07  8.889613  8.813412   \n",
       "17    158.793580  107.954988  1.794948  0.000000e+00  8.861766  8.786335   \n",
       "18    158.650688  107.836987  1.793455  0.000000e+00  8.836582  8.762949   \n",
       "19    158.595398  107.762207  1.800354  0.000000e+00  8.815814  8.744519   \n",
       "20    158.617170  107.794089  1.800437  0.000000e+00  8.819560  8.751075   \n",
       "21    158.570603  107.739285  1.797501  0.000000e+00  8.829951  8.760345   \n",
       "22    158.532522  107.730179  1.791381  0.000000e+00  8.833388  8.763846   \n",
       "23    158.509012  107.730169  1.792493  0.000000e+00  8.825094  8.754625   \n",
       "24    158.444608  107.712051  1.792617  0.000000e+00  8.814211  8.743425   \n",
       "25    158.336282  107.592135  1.738895  0.000000e+00  8.799729  8.730549   \n",
       "26    158.265087  107.516762  1.731176  0.000000e+00  8.796409  8.727124   \n",
       "27    158.251938  107.483985  1.740613  0.000000e+00  8.791439  8.722814   \n",
       "28    158.090427  107.348451  1.740558  0.000000e+00  8.781653  8.714268   \n",
       "29    157.950735  107.234886  1.734965  0.000000e+00  8.778932  8.711481   \n",
       "...          ...         ...       ...           ...       ...       ...   \n",
       "2175  161.342162  109.385953  1.994326  1.016939e+01  9.939639  9.819146   \n",
       "2176  161.261893  109.597479  1.990501  1.016826e+01  9.937849  9.818073   \n",
       "2177  161.248850  109.606825  1.989632  1.016965e+01  9.939149  9.819853   \n",
       "2178  161.257885  109.628537  1.990838  1.016666e+01  9.937847  9.817180   \n",
       "2179  161.238328  109.599679  1.994758  1.016783e+01  9.938892  9.818273   \n",
       "2180  161.275372  109.639450  2.011162  1.017330e+01  9.943790  9.823106   \n",
       "2181  161.267652  109.642587  2.009996  1.017915e+01  9.950305  9.829251   \n",
       "2182  161.260115  109.639631  1.998455  1.018283e+01  9.954449  9.833994   \n",
       "2183  161.256615  109.641287  2.007104  1.018351e+01  9.952899  9.832450   \n",
       "2184  161.257928  109.616252  2.004570  1.018874e+01  9.958713  9.838105   \n",
       "2185  161.262697  109.617288  2.000572  1.018918e+01  9.958158  9.837278   \n",
       "2186  161.205853  109.776600  1.993403  1.019087e+01  9.958828  9.838262   \n",
       "2187  161.221512  109.760709  2.004699  1.019057e+01  9.960782  9.840654   \n",
       "2188  161.206088  109.753829  2.000702  1.019067e+01  9.961115  9.841230   \n",
       "2189  161.223148  109.780724  2.002671  1.019676e+01  9.966036  9.846218   \n",
       "2190  161.213558  109.773643  1.989976  1.019673e+01  9.965220  9.846029   \n",
       "2191  161.217620  109.787039  2.002197  1.019733e+01  9.966252  9.845828   \n",
       "2192  161.234593  109.796593  2.006670  1.020063e+01  9.969385  9.848438   \n",
       "2193  161.226722  109.791730  1.993758  1.020210e+01  9.970461  9.849650   \n",
       "2194  161.227563  109.815596  2.001899  1.020436e+01  9.972642  9.851916   \n",
       "2195  161.207317  109.759292  2.002790  1.020200e+01  9.971289  9.850517   \n",
       "2196  161.226700  109.788356  2.001049  1.020408e+01  9.973693  9.851981   \n",
       "2197  161.235612  109.799786  2.000534  1.020598e+01  9.977087  9.854922   \n",
       "2198  161.244217  109.807411  1.996837  1.020707e+01  9.978510  9.854936   \n",
       "2199  161.235373  109.801761  1.994050  1.020539e+01  9.976495  9.854474   \n",
       "2200  161.227572  109.779581  2.001438  1.020247e+01  9.972037  9.850361   \n",
       "2201  161.206070  109.787481  1.998781  1.019792e+01  9.966184  9.844854   \n",
       "2202  161.192120  109.756174  1.993436  1.019682e+01  9.964329  9.842628   \n",
       "2203  161.208917  109.793884  2.007077  1.019859e+01  9.968232  9.846690   \n",
       "2204  161.217128  109.792177  2.002690  1.020313e+01  9.973638  9.851949   \n",
       "\n",
       "           FS1        FS2        TS1        TS2        TS3        TS4  \\\n",
       "0     6.709815  10.304592  35.621983  40.978767  38.471017  31.745250   \n",
       "1     6.715315  10.403098  36.676967  41.532767  38.978967  34.493867   \n",
       "2     6.718522  10.366250  37.880800  42.442450  39.631950  35.646150   \n",
       "3     6.720565  10.302678  38.879050  43.403983  40.403383  36.579467   \n",
       "4     6.690308  10.237750  39.803917  44.332750  41.310550  37.427900   \n",
       "5     6.699023  10.178720  40.659450  45.170617  42.124117  38.212067   \n",
       "6     6.698573  10.140810  41.463633  45.947233  42.888117  38.932100   \n",
       "7     6.678027  10.095978  42.215267  46.665233  43.519883  39.558967   \n",
       "8     6.671652  10.039710  42.891983  47.339433  44.201250  40.080533   \n",
       "9     6.659990   9.997762  43.532833  47.981417  44.797133  40.612550   \n",
       "10    6.659208   9.964415  44.106117  48.586267  45.365083  41.100200   \n",
       "11    6.663307   9.931453  44.638500  49.114217  45.921483  41.567850   \n",
       "12    6.655927   9.914133  45.159600  49.602167  46.391067  42.038333   \n",
       "13    6.665410   9.891230  45.628733  50.064267  46.868167  42.439717   \n",
       "14    6.669133   9.856932  46.059183  50.509083  47.318267  42.798933   \n",
       "15    6.649888   9.835402  46.462250  50.915233  47.708633  43.151967   \n",
       "16    6.649658   9.799335  46.839567  51.298867  48.062650  43.445533   \n",
       "17    6.639613   9.779605  47.152183  51.653200  48.444333  43.732033   \n",
       "18    6.660112   9.760647  47.469767  51.996400  48.740183  44.044000   \n",
       "19    6.648780   9.741035  47.751067  52.313583  49.086183  44.303867   \n",
       "20    6.638245   9.695862  48.026200  52.584767  49.347150  44.498483   \n",
       "21    6.644133   9.656905  48.270950  52.840050  49.626033  44.668517   \n",
       "22    6.636838   9.632158  48.499567  53.062083  49.856117  44.855267   \n",
       "23    6.632770   9.616863  48.711317  53.256350  50.075883  45.054550   \n",
       "24    6.634750   9.606637  48.919533  53.462100  50.269483  45.208033   \n",
       "25    6.647125   9.595623  49.110033  53.673817  50.522350  45.322150   \n",
       "26    6.634792   9.574230  49.318750  53.870583  50.721200  45.496317   \n",
       "27    6.646880   9.554103  49.495233  54.059600  50.910500  45.538167   \n",
       "28    6.641190   9.538200  49.663633  54.205583  51.065333  45.770817   \n",
       "29    6.644685   9.530807  49.824417  54.384833  51.162533  45.799083   \n",
       "...        ...        ...        ...        ...        ...        ...   \n",
       "2175  6.674418  10.171945  35.512967  41.120817  38.435850  30.533133   \n",
       "2176  6.688022  10.170250  35.512417  41.110850  38.428750  30.545017   \n",
       "2177  6.695073  10.169055  35.496467  41.101083  38.436983  30.546517   \n",
       "2178  6.684483  10.168733  35.473150  41.085917  38.423167  30.558133   \n",
       "2179  6.683057  10.169105  35.472167  41.080517  38.421583  30.553217   \n",
       "2180  6.681332  10.171602  35.450417  41.062650  38.406117  30.526983   \n",
       "2181  6.684300  10.177833  35.452183  41.048133  38.410150  30.509100   \n",
       "2182  6.687558  10.179920  35.447983  41.042150  38.403117  30.530067   \n",
       "2183  6.682443  10.177362  35.442933  41.045183  38.392933  30.485583   \n",
       "2184  6.697163  10.180568  35.433333  41.022533  38.390650  30.473633   \n",
       "2185  6.685530  10.179587  35.411450  41.020750  38.378167  30.498017   \n",
       "2186  6.694062  10.178625  35.404933  41.019383  38.386100  30.538517   \n",
       "2187  6.693118  10.180423  35.397683  41.007183  38.378233  30.443167   \n",
       "2188  6.691213  10.178522  35.391017  41.007283  38.362933  30.528667   \n",
       "2189  6.694732  10.181055  35.380583  40.994550  38.356117  30.474417   \n",
       "2190  6.695208  10.182797  35.365017  40.979983  38.348617  30.473450   \n",
       "2191  6.695542  10.182977  35.381267  40.986917  38.349483  30.452700   \n",
       "2192  6.691490  10.185430  35.365783  40.979500  38.319183  30.517733   \n",
       "2193  6.695933  10.186798  35.355933  40.972617  38.319350  30.437733   \n",
       "2194  6.694377  10.186897  35.357433  40.977750  38.319083  30.475400   \n",
       "2195  6.695060  10.185922  35.342917  40.962750  38.318167  30.404183   \n",
       "2196  6.693697  10.186425  35.326817  40.921317  38.311217  30.417400   \n",
       "2197  6.698213  10.187255  35.330217  40.913650  38.303450  30.486117   \n",
       "2198  6.698467  10.189285  35.333033  40.922583  38.282083  30.420950   \n",
       "2199  6.693777  10.187475  35.320817  40.900950  38.276067  30.462950   \n",
       "2200  6.689930  10.184515  35.313783  40.874800  38.269267  30.404733   \n",
       "2201  6.692182  10.177767  35.321600  40.868883  38.268250  30.416233   \n",
       "2202  6.693277  10.176172  35.319183  40.875950  38.246367  30.426250   \n",
       "2203  6.684128  10.178353  35.324767  40.876067  38.245733  30.414283   \n",
       "2204  6.692302  10.183393  35.322233  40.859400  38.248917  30.390800   \n",
       "\n",
       "               P1       VS1        CE1       CP1        SE1  \n",
       "0     2538.929167  0.576950  39.601350  1.862750  59.157183  \n",
       "1     2531.498900  0.565850  25.786433  1.255550  59.335617  \n",
       "2     2519.928000  0.576533  22.218233  1.113217  59.543150  \n",
       "3     2511.541633  0.569267  20.459817  1.062150  59.794900  \n",
       "4     2503.449500  0.577367  19.787017  1.070467  59.455267  \n",
       "5     2501.007067  0.572683  19.149683  1.072083  59.563333  \n",
       "6     2494.416900  0.573033  18.666383  1.081683  59.789900  \n",
       "7     2489.421533  0.572000  18.178433  1.078700  59.590617  \n",
       "8     2484.419067  0.567067  18.334867  1.115083  59.608883  \n",
       "9     2480.434867  0.571683  18.205733  1.129133  59.473733  \n",
       "10    2478.794900  0.568700  18.112017  1.145350  59.584350  \n",
       "11    2472.273200  0.578150  18.068833  1.164583  59.758183  \n",
       "12    2467.228833  0.584233  17.742283  1.162967  59.736817  \n",
       "13    2465.307167  0.588567  17.678150  1.179900  59.916483  \n",
       "14    2461.772333  0.582467  17.778833  1.201867  60.047317  \n",
       "15    2459.511367  0.582367  17.692367  1.207383  59.811100  \n",
       "16    2458.166100  0.584900  17.721517  1.219617  59.866617  \n",
       "17    2455.588233  0.593083  17.820150  1.242183  59.768017  \n",
       "18    2452.299767  0.603900  17.555983  1.235317  60.096300  \n",
       "19    2448.326333  0.603350  17.664367  1.256117  60.018900  \n",
       "20    2444.721633  0.618667  17.734167  1.266883  59.941550  \n",
       "21    2441.775800  0.630700  17.942617  1.290500  60.187200  \n",
       "22    2438.771200  0.621733  17.955650  1.297683  60.082567  \n",
       "23    2437.650633  0.625033  17.929517  1.302283  59.959200  \n",
       "24    2437.126767  0.633300  17.960767  1.310467  60.043717  \n",
       "25    2434.605600  0.636117  18.289017  1.344800  60.350783  \n",
       "26    2432.505433  0.637733  18.268217  1.348583  60.216483  \n",
       "27    2432.652367  0.626700  18.646100  1.383417  60.431350  \n",
       "28    2428.354700  0.635050  18.258217  1.359417  60.359350  \n",
       "29    2425.844033  0.633967  18.431567  1.375567  60.390467  \n",
       "...           ...       ...        ...       ...        ...  \n",
       "2175  2540.648400  0.555833  46.683000  2.167067  58.708683  \n",
       "2176  2538.508867  0.547800  46.583217  2.161067  59.101483  \n",
       "2177  2538.181900  0.554417  46.602800  2.162650  59.184833  \n",
       "2178  2538.119000  0.555717  46.487783  2.155467  59.014083  \n",
       "2179  2538.206833  0.556367  46.494200  2.155100  58.968567  \n",
       "2180  2539.548433  0.548417  46.590033  2.158650  58.885300  \n",
       "2181  2543.212000  0.551317  46.713483  2.166683  58.894067  \n",
       "2182  2543.497533  0.547600  46.589900  2.160583  58.908850  \n",
       "2183  2543.064467  0.549333  46.802450  2.168850  58.888483  \n",
       "2184  2544.166600  0.546300  46.835433  2.172167  59.058083  \n",
       "2185  2543.716533  0.547017  46.568033  2.160483  58.878683  \n",
       "2186  2542.299033  0.547483  46.220433  2.152333  59.067717  \n",
       "2187  2544.374667  0.548450  46.735950  2.176400  59.049367  \n",
       "2188  2542.845567  0.546333  46.185100  2.149250  59.031867  \n",
       "2189  2545.004300  0.549417  46.492533  2.163483  59.043150  \n",
       "2190  2544.815000  0.552883  46.530900  2.160183  59.035683  \n",
       "2191  2545.049367  0.551367  46.581917  2.164700  59.068667  \n",
       "2192  2545.462667  0.549500  46.099400  2.141167  58.979917  \n",
       "2193  2545.667933  0.551733  46.565767  2.161533  59.043500  \n",
       "2194  2546.162167  0.553383  46.352417  2.152417  59.024267  \n",
       "2195  2546.288767  0.550417  46.900667  2.171900  59.045667  \n",
       "2196  2548.492433  0.552767  46.930483  2.165933  58.995550  \n",
       "2197  2547.434767  0.550067  46.289217  2.147083  59.053533  \n",
       "2198  2546.977033  0.549300  46.552917  2.158850  59.068100  \n",
       "2199  2547.040400  0.551517  46.308350  2.146750  59.007067  \n",
       "2200  2543.911033  0.550833  46.628517  2.160600  59.033100  \n",
       "2201  2543.411333  0.547483  46.689817  2.151450  59.068000  \n",
       "2202  2542.729767  0.545233  46.472300  2.143300  59.132350  \n",
       "2203  2544.046333  0.537017  46.544967  2.148483  58.970800  \n",
       "2204  2543.818300  0.546583  46.647933  2.157050  59.053900  \n",
       "\n",
       "[2205 rows x 17 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_pickle(\"Y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 :Cooler condition / %:\n",
    "# \t3: close to total failure\n",
    "# \t20: reduced effifiency\n",
    "# \t100: full efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2205.000000\n",
       "mean       41.240816\n",
       "std        42.383143\n",
       "min         3.000000\n",
       "25%         3.000000\n",
       "50%        20.000000\n",
       "75%       100.000000\n",
       "max       100.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0\n",
      "0       3\n",
      "1       3\n",
      "2       3\n",
      "3       3\n",
      "4       3\n",
      "5       3\n",
      "6       3\n",
      "7       3\n",
      "8       3\n",
      "9       3\n",
      "10      3\n",
      "11      3\n",
      "12      3\n",
      "13      3\n",
      "14      3\n",
      "15      3\n",
      "16      3\n",
      "17      3\n",
      "18      3\n",
      "19      3\n",
      "20      3\n",
      "21      3\n",
      "22      3\n",
      "23      3\n",
      "24      3\n",
      "25      3\n",
      "26      3\n",
      "27      3\n",
      "28      3\n",
      "29      3\n",
      "...   ...\n",
      "2175  100\n",
      "2176  100\n",
      "2177  100\n",
      "2178  100\n",
      "2179  100\n",
      "2180  100\n",
      "2181  100\n",
      "2182  100\n",
      "2183  100\n",
      "2184  100\n",
      "2185  100\n",
      "2186  100\n",
      "2187  100\n",
      "2188  100\n",
      "2189  100\n",
      "2190  100\n",
      "2191  100\n",
      "2192  100\n",
      "2193  100\n",
      "2194  100\n",
      "2195  100\n",
      "2196  100\n",
      "2197  100\n",
      "2198  100\n",
      "2199  100\n",
      "2200  100\n",
      "2201  100\n",
      "2202  100\n",
      "2203  100\n",
      "2204  100\n",
      "\n",
      "[2205 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "target = pd.DataFrame(Y[0])\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.loc[(target[0] == 3)] = 0\n",
    "target.loc[(target[0] == 20)] = 1\n",
    "target.loc[(target[0] == 100)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "...  ..\n",
      "2175  2\n",
      "2176  2\n",
      "2177  2\n",
      "2178  2\n",
      "2179  2\n",
      "2180  2\n",
      "2181  2\n",
      "2182  2\n",
      "2183  2\n",
      "2184  2\n",
      "2185  2\n",
      "2186  2\n",
      "2187  2\n",
      "2188  2\n",
      "2189  2\n",
      "2190  2\n",
      "2191  2\n",
      "2192  2\n",
      "2193  2\n",
      "2194  2\n",
      "2195  2\n",
      "2196  2\n",
      "2197  2\n",
      "2198  2\n",
      "2199  2\n",
      "2200  2\n",
      "2201  2\n",
      "2202  2\n",
      "2203  2\n",
      "2204  2\n",
      "\n",
      "[2205 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(features_df, target_df, sequence_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds sequences from data and converts them into pytorch tensors\n",
    "    sequence_length - represents the number of samples to be considered in a sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    data_ = []\n",
    "    target_ = []\n",
    "    \n",
    "    for i in range(int(features_df.shape[0]/sequence_length)):\n",
    "        \n",
    "        data = torch.from_numpy(features_df.iloc[i:i+sequence_length].values)\n",
    "        target = torch.from_numpy(target_df.iloc[i+sequence_length+1].values)\n",
    "        \n",
    "        data_.append(data)\n",
    "        target_.append(target)\n",
    "        \n",
    "    data = torch.stack(data_)\n",
    "    target = torch.stack(target_)\n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[160.6735, 109.4669,   1.9915,  ...,  39.6013,   1.8627,  59.1572],\n",
      "         [160.6033, 109.3549,   1.9762,  ...,  25.7864,   1.2555,  59.3356],\n",
      "         [160.3477, 109.1588,   1.9722,  ...,  22.2182,   1.1132,  59.5432]],\n",
      "\n",
      "        [[160.6033, 109.3549,   1.9762,  ...,  25.7864,   1.2555,  59.3356],\n",
      "         [160.3477, 109.1588,   1.9722,  ...,  22.2182,   1.1132,  59.5432],\n",
      "         [160.1881, 109.0648,   1.9466,  ...,  20.4598,   1.0622,  59.7949]],\n",
      "\n",
      "        [[160.3477, 109.1588,   1.9722,  ...,  22.2182,   1.1132,  59.5432],\n",
      "         [160.1881, 109.0648,   1.9466,  ...,  20.4598,   1.0622,  59.7949],\n",
      "         [160.0005, 108.9314,   1.9227,  ...,  19.7870,   1.0705,  59.4553]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[156.3950, 105.6824,   1.7380,  ...,  22.7522,   1.7663,  59.9804],\n",
      "         [156.5137, 105.7913,   1.7412,  ...,  27.2730,   2.1196,  59.8352],\n",
      "         [156.6206, 105.8789,   1.7347,  ...,  27.8870,   2.1560,  59.9289]],\n",
      "\n",
      "        [[156.5137, 105.7913,   1.7412,  ...,  27.2730,   2.1196,  59.8352],\n",
      "         [156.6206, 105.8789,   1.7347,  ...,  27.8870,   2.1560,  59.9289],\n",
      "         [156.7411, 105.9706,   1.7399,  ...,  28.0845,   2.1481,  59.8620]],\n",
      "\n",
      "        [[156.6206, 105.8789,   1.7347,  ...,  27.8870,   2.1560,  59.9289],\n",
      "         [156.7411, 105.9706,   1.7399,  ...,  28.0845,   2.1481,  59.8620],\n",
      "         [156.7985, 106.0043,   1.7433,  ...,  28.0021,   2.1213,  59.7370]]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([735, 3, 17])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 3\n",
    "\n",
    "features = X\n",
    "target = target\n",
    "data, target = build_sequences(features, target, sequence_length=sequence_length)\n",
    "\n",
    "print(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.6067e+02, 1.0947e+02, 1.9915e+00],\n",
      "         [0.0000e+00, 9.8422e+00, 9.7281e+00],\n",
      "         [6.7098e+00, 1.0305e+01, 3.5622e+01],\n",
      "         ...,\n",
      "         [3.7881e+01, 4.2442e+01, 3.9632e+01],\n",
      "         [3.5646e+01, 2.5199e+03, 5.7653e-01],\n",
      "         [2.2218e+01, 1.1132e+00, 5.9543e+01]],\n",
      "\n",
      "        [[1.6060e+02, 1.0935e+02, 1.9762e+00],\n",
      "         [0.0000e+00, 9.6351e+00, 9.5295e+00],\n",
      "         [6.7153e+00, 1.0403e+01, 3.6677e+01],\n",
      "         ...,\n",
      "         [3.8879e+01, 4.3404e+01, 4.0403e+01],\n",
      "         [3.6579e+01, 2.5115e+03, 5.6927e-01],\n",
      "         [2.0460e+01, 1.0622e+00, 5.9795e+01]],\n",
      "\n",
      "        [[1.6035e+02, 1.0916e+02, 1.9722e+00],\n",
      "         [0.0000e+00, 9.5305e+00, 9.4279e+00],\n",
      "         [6.7185e+00, 1.0366e+01, 3.7881e+01],\n",
      "         ...,\n",
      "         [3.9804e+01, 4.4333e+01, 4.1311e+01],\n",
      "         [3.7428e+01, 2.5034e+03, 5.7737e-01],\n",
      "         [1.9787e+01, 1.0705e+00, 5.9455e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.5639e+02, 1.0568e+02, 1.7380e+00],\n",
      "         [0.0000e+00, 8.5014e+00, 8.4463e+00],\n",
      "         [6.5799e+00, 9.0306e+00, 5.5617e+01],\n",
      "         ...,\n",
      "         [5.4040e+01, 5.9246e+01, 5.6529e+01],\n",
      "         [4.7723e+01, 2.3908e+03, 6.1255e-01],\n",
      "         [2.7887e+01, 2.1560e+00, 5.9929e+01]],\n",
      "\n",
      "        [[1.5651e+02, 1.0579e+02, 1.7412e+00],\n",
      "         [0.0000e+00, 8.5135e+00, 8.4574e+00],\n",
      "         [6.5726e+00, 9.0453e+00, 5.4834e+01],\n",
      "         ...,\n",
      "         [5.3403e+01, 5.8632e+01, 5.5969e+01],\n",
      "         [4.7239e+01, 2.3952e+03, 6.0448e-01],\n",
      "         [2.8085e+01, 2.1481e+00, 5.9862e+01]],\n",
      "\n",
      "        [[1.5662e+02, 1.0588e+02, 1.7347e+00],\n",
      "         [0.0000e+00, 8.5501e+00, 8.4926e+00],\n",
      "         [6.5875e+00, 9.0837e+00, 5.4040e+01],\n",
      "         ...,\n",
      "         [5.2867e+01, 5.8121e+01, 5.5492e+01],\n",
      "         [4.6898e+01, 2.4004e+03, 6.0815e-01],\n",
      "         [2.8002e+01, 2.1213e+00, 5.9737e+01]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# data.transpose(1,2)\n",
    "data = data.reshape(735,17,3)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([735, 17, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([735, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(target)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.7292e+02, 1.2248e+02, 1.0827e+00],\n",
      "         [0.0000e+00, 8.3934e+00, 8.3488e+00],\n",
      "         [3.2903e+00, 8.9107e+00, 5.7615e+01],\n",
      "         [6.0986e+01, 5.9243e+01, 5.2896e+01],\n",
      "         [2.6217e+03, 7.2072e-01, 1.9169e+01],\n",
      "         [1.5244e+00, 2.9909e+01, 1.7289e+02],\n",
      "         [1.2246e+02, 1.0835e+00, 0.0000e+00],\n",
      "         [8.3915e+00, 8.3470e+00, 3.2877e+00],\n",
      "         [8.9090e+00, 5.7627e+01, 6.0993e+01],\n",
      "         [5.9287e+01, 5.2964e+01, 2.6171e+03],\n",
      "         [7.1813e-01, 1.9099e+01, 1.5167e+00],\n",
      "         [2.9894e+01, 1.7293e+02, 1.2249e+02],\n",
      "         [1.0846e+00, 0.0000e+00, 8.3920e+00],\n",
      "         [8.3477e+00, 3.2818e+00, 8.9108e+00],\n",
      "         [5.7634e+01, 6.1004e+01, 5.9282e+01],\n",
      "         [5.2972e+01, 2.6180e+03, 7.1567e-01],\n",
      "         [1.9097e+01, 1.5154e+00, 2.9785e+01]],\n",
      "\n",
      "        [[1.7306e+02, 1.2283e+02, 1.0756e+00],\n",
      "         [3.4296e-02, 8.4033e+00, 8.3598e+00],\n",
      "         [3.2793e+00, 8.9357e+00, 5.7103e+01],\n",
      "         [6.0267e+01, 5.8816e+01, 5.2526e+01],\n",
      "         [2.6374e+03, 7.5920e-01, 1.9317e+01],\n",
      "         [1.5146e+00, 2.9354e+01, 1.7260e+02],\n",
      "         [1.2280e+02, 1.0790e+00, 8.1855e-02],\n",
      "         [8.4007e+00, 8.3582e+00, 3.2891e+00],\n",
      "         [8.9340e+00, 5.7109e+01, 6.0395e+01],\n",
      "         [5.8867e+01, 5.2641e+01, 2.6300e+03],\n",
      "         [7.2242e-01, 1.9047e+01, 1.4983e+00],\n",
      "         [3.0398e+01, 1.7259e+02, 1.2280e+02],\n",
      "         [1.0814e+00, 1.5045e-03, 8.4044e+00],\n",
      "         [8.3616e+00, 3.2806e+00, 8.9345e+00],\n",
      "         [5.7218e+01, 6.0471e+01, 5.8842e+01],\n",
      "         [5.2671e+01, 2.6312e+03, 7.2520e-01],\n",
      "         [1.8896e+01, 1.4843e+00, 3.0265e+01]],\n",
      "\n",
      "        [[1.7309e+02, 1.2255e+02, 1.0873e+00],\n",
      "         [1.3233e-04, 8.4499e+00, 8.4005e+00],\n",
      "         [3.2311e+00, 8.9936e+00, 5.6279e+01],\n",
      "         [6.0317e+01, 5.7834e+01, 5.1881e+01],\n",
      "         [2.6302e+03, 7.3832e-01, 1.8654e+01],\n",
      "         [1.4442e+00, 2.9275e+01, 1.7306e+02],\n",
      "         [1.2252e+02, 1.0965e+00, 1.2404e-02],\n",
      "         [8.4503e+00, 8.4008e+00, 3.2265e+00],\n",
      "         [8.9923e+00, 5.6277e+01, 6.0125e+01],\n",
      "         [5.7824e+01, 5.1580e+01, 2.6283e+03],\n",
      "         [7.2812e-01, 1.9567e+01, 1.5138e+00],\n",
      "         [2.9195e+01, 1.7302e+02, 1.2250e+02],\n",
      "         [1.0968e+00, 1.2889e-02, 8.4454e+00],\n",
      "         [8.3949e+00, 3.2305e+00, 8.9868e+00],\n",
      "         [5.6271e+01, 6.0010e+01, 5.7849e+01],\n",
      "         [5.1688e+01, 2.6275e+03, 7.4188e-01],\n",
      "         [1.9295e+01, 1.4918e+00, 2.9254e+01]],\n",
      "\n",
      "        [[1.5740e+02, 1.0674e+02, 1.7268e+00],\n",
      "         [0.0000e+00, 8.6702e+00, 8.6128e+00],\n",
      "         [6.6236e+00, 9.3260e+00, 5.2210e+01],\n",
      "         [5.6863e+01, 5.3737e+01, 4.7755e+01],\n",
      "         [2.4068e+03, 6.4663e-01, 1.9518e+01],\n",
      "         [1.5024e+00, 6.0557e+01, 1.5734e+02],\n",
      "         [1.0668e+02, 1.7240e+00, 0.0000e+00],\n",
      "         [8.6690e+00, 8.6118e+00, 6.6215e+00],\n",
      "         [9.3304e+00, 5.2305e+01, 5.6874e+01],\n",
      "         [5.3824e+01, 4.7884e+01, 2.4092e+03],\n",
      "         [6.4582e-01, 1.9709e+01, 1.4917e+00],\n",
      "         [6.0402e+01, 1.5733e+02, 1.0667e+02],\n",
      "         [1.7246e+00, 0.0000e+00, 8.6702e+00],\n",
      "         [8.6138e+00, 6.6167e+00, 9.3297e+00],\n",
      "         [5.2378e+01, 5.6898e+01, 5.3821e+01],\n",
      "         [4.7888e+01, 2.4094e+03, 6.4003e-01],\n",
      "         [1.9857e+01, 1.4922e+00, 6.0341e+01]],\n",
      "\n",
      "        [[1.7286e+02, 1.2318e+02, 1.1172e+00],\n",
      "         [0.0000e+00, 8.4049e+00, 8.3587e+00],\n",
      "         [3.3024e+00, 8.9113e+00, 5.7099e+01],\n",
      "         [6.0641e+01, 5.8672e+01, 5.2504e+01],\n",
      "         [2.6049e+03, 7.1877e-01, 1.8664e+01],\n",
      "         [1.4810e+00, 3.0811e+01, 1.7286e+02],\n",
      "         [1.2317e+02, 1.1152e+00, 0.0000e+00],\n",
      "         [8.4029e+00, 8.3573e+00, 3.3029e+00],\n",
      "         [8.9113e+00, 5.7079e+01, 6.0597e+01],\n",
      "         [5.8720e+01, 5.2518e+01, 2.6053e+03],\n",
      "         [7.0990e-01, 1.8725e+01, 1.4902e+00],\n",
      "         [3.0815e+01, 1.7279e+02, 1.2311e+02],\n",
      "         [1.1220e+00, 1.5380e-03, 8.4010e+00],\n",
      "         [8.3563e+00, 3.3007e+00, 8.9114e+00],\n",
      "         [5.7114e+01, 6.0581e+01, 5.8721e+01],\n",
      "         [5.2518e+01, 2.6044e+03, 7.2085e-01],\n",
      "         [1.8717e+01, 1.4899e+00, 3.0764e+01]],\n",
      "\n",
      "        [[1.5626e+02, 1.0511e+02, 1.6865e+00],\n",
      "         [0.0000e+00, 8.4093e+00, 8.3599e+00],\n",
      "         [6.5727e+00, 8.9330e+00, 5.7330e+01],\n",
      "         [6.1493e+01, 5.8674e+01, 5.2563e+01],\n",
      "         [2.3673e+03, 7.2737e-01, 1.8883e+01],\n",
      "         [1.4719e+00, 5.9950e+01, 1.7331e+02],\n",
      "         [1.2306e+02, 1.1060e+00, 7.2383e-02],\n",
      "         [8.4060e+00, 8.3563e+00, 3.4192e+00],\n",
      "         [8.9299e+00, 5.7177e+01, 6.1278e+01],\n",
      "         [5.8705e+01, 5.2547e+01, 2.5979e+03],\n",
      "         [7.3612e-01, 1.9007e+01, 1.4825e+00],\n",
      "         [3.1829e+01, 1.7332e+02, 1.2309e+02],\n",
      "         [1.1176e+00, 0.0000e+00, 8.4012e+00],\n",
      "         [8.3516e+00, 3.4310e+00, 8.9198e+00],\n",
      "         [5.7209e+01, 6.1084e+01, 5.8781e+01],\n",
      "         [5.2560e+01, 2.5974e+03, 7.7435e-01],\n",
      "         [1.9135e+01, 1.4932e+00, 3.2041e+01]],\n",
      "\n",
      "        [[1.5696e+02, 1.0627e+02, 1.7369e+00],\n",
      "         [0.0000e+00, 8.5404e+00, 8.4820e+00],\n",
      "         [6.5978e+00, 9.1899e+00, 5.4155e+01],\n",
      "         [5.8774e+01, 5.5775e+01, 4.9420e+01],\n",
      "         [2.3973e+03, 6.5790e-01, 2.0258e+01],\n",
      "         [1.5743e+00, 6.0436e+01, 1.5697e+02],\n",
      "         [1.0631e+02, 1.7373e+00, 0.0000e+00],\n",
      "         [8.5369e+00, 8.4779e+00, 6.5923e+00],\n",
      "         [9.1874e+00, 5.4186e+01, 5.8790e+01],\n",
      "         [5.5764e+01, 4.9473e+01, 2.3978e+03],\n",
      "         [6.6740e-01, 2.0055e+01, 1.5564e+00],\n",
      "         [6.0339e+01, 1.5701e+02, 1.0632e+02],\n",
      "         [1.7362e+00, 0.0000e+00, 8.5387e+00],\n",
      "         [8.4801e+00, 6.5997e+00, 9.1850e+00],\n",
      "         [5.4190e+01, 5.8794e+01, 5.5796e+01],\n",
      "         [4.9494e+01, 2.3996e+03, 6.7080e-01],\n",
      "         [2.0096e+01, 1.5588e+00, 6.0444e+01]],\n",
      "\n",
      "        [[1.5643e+02, 1.0584e+02, 1.6881e+00],\n",
      "         [0.0000e+00, 8.5120e+00, 8.4545e+00],\n",
      "         [6.3697e+00, 9.1880e+00, 5.4402e+01],\n",
      "         [5.8968e+01, 5.5945e+01, 4.9707e+01],\n",
      "         [2.4029e+03, 7.2530e-01, 2.0192e+01],\n",
      "         [1.5422e+00, 5.7970e+01, 1.5640e+02],\n",
      "         [1.0584e+02, 1.6969e+00, 0.0000e+00],\n",
      "         [8.5106e+00, 8.4528e+00, 6.3821e+00],\n",
      "         [9.1828e+00, 5.4377e+01, 5.8984e+01],\n",
      "         [5.5938e+01, 4.9519e+01, 2.4027e+03],\n",
      "         [6.5723e-01, 2.0747e+01, 1.5883e+00],\n",
      "         [5.8151e+01, 1.5683e+02, 1.0612e+02],\n",
      "         [1.7540e+00, 1.2582e-02, 8.5143e+00],\n",
      "         [8.4572e+00, 6.6067e+00, 9.1890e+00],\n",
      "         [5.4414e+01, 5.9006e+01, 5.5924e+01],\n",
      "         [4.9738e+01, 2.3952e+03, 6.7907e-01],\n",
      "         [1.9990e+01, 1.5305e+00, 6.0531e+01]]], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 17, 3])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test data split\n",
    "test_size = 0.2\n",
    "\n",
    "indices = torch.randperm(data.shape[0])\n",
    "\n",
    "train_indices = indices[:int(indices.shape[0] * (1-test_size))]\n",
    "test_indices = indices[int(indices.shape[0] * (1-test_size)):]\n",
    "\n",
    "X_train, y_train = data[train_indices], target[train_indices]\n",
    "X_test, y_test = data[test_indices], target[test_indices]\n",
    "\n",
    "print(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hydraulic_Dataset(torch.utils.data.dataset.Dataset):\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "hs_train_dataset = Hydraulic_Dataset(X_train, y_train)\n",
    "hs_train_loader = torch.utils.data.dataloader.DataLoader(hs_train_dataset, batch_size= batch_size)\n",
    "\n",
    "hs_test_dataset = Hydraulic_Dataset(X_test, y_test)\n",
    "hs_test_loader = torch.utils.data.dataloader.DataLoader(hs_test_dataset, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, sequence_length, n_features):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(17, 32, kernel_size= 3)\n",
    "        \n",
    "        self.lin_in_size = self.conv1.out_channels * int(((sequence_length - (self.conv1.kernel_size[0]-1) -1)/self.conv1.stride[0] +1))\n",
    "        print(self.lin_in_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.lin_in_size,64)\n",
    "        self.fc2 = nn.Linear(64, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, self.lin_in_size)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        log_probs= torch.nn.functional.log_softmax(x, dim=1)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv1d(17, 32, kernel_size=(3,), stride=(1,))\n",
       "  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 17\n",
    "print(n_features)\n",
    "net = Network(sequence_length, n_features).double()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 2. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 3. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 4. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 5. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 6. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 7. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 8. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 9. Loss: 0.0. Accuracy: 100.0\n",
      "Epoch: 10. Loss: 0.0. Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "training_losses = []\n",
    "training_accuracy=[]\n",
    "a=[]\n",
    "b=[]\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    batch_accuracy=[]\n",
    "    batch_losses = []\n",
    "    for i, (data, target) in enumerate(hs_train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = net(data)\n",
    "        loss = criterion(out, target.squeeze())\n",
    "        batch_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        prediction = torch.round(out.data)   \n",
    "        \n",
    "# softmax = torch.exp(output).cpu()\n",
    "# prob = list(softmax.numpy())\n",
    "# predictions = np.argmax(prob, axis=1)\n",
    "        \n",
    "        \n",
    "        a1=prediction.tolist()\n",
    "        a2=a+a1\n",
    "        a=a2\n",
    "        \n",
    "        b1=target.tolist()\n",
    "        b2=b+b1\n",
    "        b=b2\n",
    "        \n",
    "        \n",
    "        total= 0\n",
    "        correct = 0\n",
    "        total += target.size(0)  \n",
    "        out = torch.round(out)\n",
    "        correct += (out == target).sum().item()\n",
    "        accuracy = 100 * (correct / total)\n",
    "        \n",
    "        batch_accuracy.append(accuracy)\n",
    "    training_losses.append(np.mean(batch_losses))\n",
    "    training_accuracy.append(np.mean(batch_accuracy))\n",
    "    print('Epoch: {}. Loss: {}. Accuracy: {}'.format(epoch+1, training_losses[-1], training_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a7ab75c6a0>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_accuracy,'r',label = \"training_accuracy\")\n",
    "plt.legend()\n",
    "plt.title('CNN')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('training_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#Creating list of all outputs and targets obtained after each epoch of training\n",
    "out=list(itertools.chain.from_iterable(a2))\n",
    "target=list(itertools.chain.from_iterable(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [80, 240]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-f0a632a9f8e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'CNN sigmoid Training Confusion Matrix :'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 205\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [80, 240]"
     ]
    }
   ],
   "source": [
    "#Calculation of classification matrix for training\n",
    "#Creation of classification report for training\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "results = confusion_matrix(target, out) \n",
    "print ('CNN sigmoid Training Confusion Matrix :')\n",
    "print(results) \n",
    "print ('CNN sigmoid Training Accuracy Score :',accuracy_score(target, out) )\n",
    "print ('CNN sigmoid Training Report : ')\n",
    "print (classification_report(target, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for graphical respresentation of confusion matrix for training \n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize =False, title = \"Confusion Matrix\" ):\n",
    "    print (\"Confusion matrix\")\n",
    "    \n",
    "    print (cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=\"Blues\")\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(-0.5,len(classes),1)\n",
    "    plt.xticks(tick_marks, (classes))\n",
    "    plt.yticks(tick_marks, (classes))\n",
    "    fmt = \"d\"\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-ab9fbef758ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graphical respresentation of confusion matrix for training\n",
    "\n",
    "classes = ['0','1']\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(results, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-4e353fcb5a5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1832\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[1;31mIndexError\u001b[0m: dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "outputs = []\n",
    "\n",
    "testing_loss = []\n",
    "batch_loss = []\n",
    "\n",
    "testing_accuracy = []\n",
    "test_batch_accuracy=[]\n",
    "\n",
    "a_1=[]\n",
    "b_1=[]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(hs_test_loader):\n",
    "\n",
    "        out = net(data)\n",
    "        loss = criterion(out, target.squeeze())\n",
    "        targets.append(target.item())\n",
    "        outputs.append(out.item())\n",
    "        batch_loss.append(loss.item())\n",
    "        print('Target : {:.4f}, Predicted Output : {:.4f}'.format(target.item(), out.item()))\n",
    "        prediction = torch.round(out.data)\n",
    "    \n",
    "        a3=prediction.tolist()\n",
    "        a4=a_1+a3\n",
    "        a_1=a4\n",
    "        \n",
    "        b3=target.tolist()\n",
    "        b4=b_1+b3\n",
    "        b_1=b4\n",
    "        \n",
    "        \n",
    "        total= 0\n",
    "        correct = 0\n",
    "        total += target.size(0)\n",
    "        out = torch.round(out)\n",
    "        correct += (out == target).sum().item()\n",
    "        accuracy = 100 * (correct / total)\n",
    "              \n",
    "    test_batch_accuracy.append(accuracy)\n",
    "    #mean of loss of all batches => Testing loss        \n",
    "    testing_loss.append(np.mean(batch_loss))\n",
    "    # mean accuracy of all batches => Testing accuracy  \n",
    "    testing_accuracy.append(np.mean(batch_accuracy))\n",
    "    \n",
    "print('Loss: {}. Accuracy: {}'.format(testing_loss[-1], testing_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testing_accuracy,'b',label = \"testing_accuracy\")\n",
    "plt.legend()\n",
    "plt.title('CNN')\n",
    "plt.ylabel('testing_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating list of all outputs and targets obtained after testing\n",
    "predicted1=list(itertools.chain.from_iterable(a4))\n",
    "targets1=list(itertools.chain.from_iterable(b4))\n",
    "\n",
    "#Calculation of classification matrix for testing\n",
    "#Creation of classification report for testing\n",
    "results1 = confusion_matrix(targets1, predicted1) \n",
    "print ('Confusion Matrix :')\n",
    "print(results1) \n",
    "print ('Accuracy Score :',accuracy_score(targets1, predicted1) )\n",
    "print ('Report : ')\n",
    "print (classification_report(targets1, predicted1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphical respresentation of confusion matrix for testing\n",
    "plot_confusion_matrix(results1, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
